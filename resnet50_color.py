from keras.layers import Input, Dense, Flatten, Reshape, Conv2D, UpSampling2D, MaxPooling2D, BatchNormalization, Concatenate
from keras.models import Model, Sequential
from keras.datasets import mnist
from keras.callbacks import Callback
from keras.optimizers import Adam
from keras import applications
import random
import glob
import wandb
from wandb.keras import WandbCallback
import subprocess
import os
from PIL import Image
import numpy as np
import cv2
from keras import backend as K
from skimage import io, color
from keras.applications.resnet50 import ResNet50

run = wandb.init(project='colorizer-applied-dl')
config = run.config

config.num_epochs = 1000
config.batch_size = 32
config.img_dir = "images"
config.height = 256
config.width = 256

val_dir = 'test'
train_dir = 'train'

# automatically get the data if it doesn't exist
if not os.path.exists("train"):
    print("Downloading flower dataset...")
    subprocess.check_output("curl https://storage.googleapis.com/l2kzone/flowers.tar | tar xz", shell=True)

def my_generator(batch_size, img_dir):
    """A generator that returns black and white images and color images"""
    image_filenames = glob.glob(img_dir + "/*")
    counter = 0
    while True:
        bw_images = np.zeros((batch_size, config.width, config.height))
        color_images = np.zeros((batch_size, config.width, config.height, 3))
        random.shuffle(image_filenames) 
        if ((counter+1)*batch_size>=len(image_filenames)):
              counter = 0
        for i in range(batch_size):
              img = Image.open(image_filenames[counter + i]).resize((config.width, config.height))
              color_images[i] = np.array(img)
              bw_images[i] = np.array(img.convert('L'))
        yield (bw_images, color_images)
        counter += batch_size


def perceptual_distance(y_true, y_pred):
    rmean = ( y_true[:,:,:,0] + y_pred[:,:,:,0] ) / 2;
    r = y_true[:,:,:,0] - y_pred[:,:,:,0]
    g = y_true[:,:,:,1] - y_pred[:,:,:,1]
    b = y_true[:,:,:,2] - y_pred[:,:,:,2]   
    return K.mean(K.sqrt((((512+rmean)*r*r)/256) + 4*g*g + (((767-rmean)*b*b)/256)));


print('Loading model with ResNet50 upto Layer 76 and appending Deconv blocks...')
img_input = Input(shape=(config.height,config.width))
img_input_bw = Reshape((config.height,config.width,1), input_shape=(config.height,config.width))(img_input)
img_conc = Concatenate()([img_input_bw, img_input_bw, img_input_bw])
resnet_model = ResNet50(input_tensor=img_conc, pooling='max', include_top=False)
intermediate_layer_output = resnet_model.layers[76].output #output shape (32, 32, 128)
conv2d_output_0 = Conv2D(filters=128, kernel_size=(3,3), activation='relu', padding='same')(intermediate_layer_output)
upsampling_output_0 = UpSampling2D(size=(2,2))(conv2d_output_0)
conv2d_output_1 = Conv2D(filters=64, kernel_size=(3,3), activation='relu', padding='same')(upsampling_output_0)
upsampling_output_1 = UpSampling2D(size=(2,2))(conv2d_output_1)
conv2d_output_2 = Conv2D(filters=32, kernel_size=(3,3), activation='relu', padding='same')(upsampling_output_1)
upsampling_output_2 = UpSampling2D(size=(2,2))(conv2d_output_2)
conv2d_output_2 = Conv2D(filters=16, kernel_size=(3,3), activation='relu', padding='same')(upsampling_output_1)
upsampling_output_2 = UpSampling2D(size=(2,2))(conv2d_output_2)
conv2d_output_2 = Conv2D(filters=8, kernel_size=(3,3), activation='relu', padding='same')(upsampling_output_1)
upsampling_output_2 = UpSampling2D(size=(2,2))(conv2d_output_2)
color_image = Conv2D(filters=3, kernel_size=(3,3),activation='relu', padding='same')(upsampling_output_2)
model_final = Model(inputs=img_input, outputs=color_image)
print('Loaded model!!!')


AB_BIN_WEIGHTS = [0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022701514488164282, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022701514488164282, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022701514488164282, 0.0022683875960096394, 0.0022701514488164282, 0.0022701514488164282, 0.0022692691796621298, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022701514488164282, 0.0022710344042723954, 0.0022701514488164282, 0.0022710344042723954, 0.0022639899323660677, 0.002260484059354882, 0.002266626482016857, 0.002267506697060341, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.002267506697060341, 0.002241394204601768, 0.0022336773300481384, 0.002264868100464008, 0.0022319696796154315, 0.0022091693402192733, 0.00215137779747977, 0.0021529643109304136, 0.002226862358492229, 0.0022578617718606478, 0.0022692691796621298, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022613595092805684, 0.0022217783577366725, 0.0021876494360075008, 0.0022008425383961473, 0.002119361883231732, 0.001908124001954105, 0.0015624533084842031, 0.0015641274780188452, 0.0018263578991269066, 0.0021124611282973037, 0.0022422549277471057, 0.002267506697060341, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022692691796621298, 0.0021721866155591757, 0.0018738325673884526, 0.0015738240091854817, 0.0016545424775093061, 0.0016419801578567428, 0.001260559793541178, 0.0007593906077023949, 0.00060332375291495, 0.000819156768135748, 0.0014648509985228488, 0.002009239879317096, 0.0022200888455365334, 0.0022613595092805684, 0.0022683875960096394, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0020581061158842207, 0.0013661451187699577, 0.0007553628351225719, 0.0005590741993200676, 0.0004547491868847851, 0.000398049236181538, 0.00023146745471483223, 0.00021732977932181793, 0.0003341530002419387, 0.0007380806653742855, 0.001466322440609064, 0.001975255039905242, 0.0021466322553506103, 0.0022311168331513747, 0.0022692691796621298, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0021577379219253423, 0.0010867164211620312, 0.00030931777182422273, 0.00013504651065775655, 7.518975659822808e-05, 3.878197354580426e-05, 2.755266024810407e-05, 8.05747510329907e-05, 0.00017891292915754315, 0.000450782049522368, 0.001029794009741643, 0.0016953766052291062, 0.0020823270502972244, 0.0022016723928287964, 0.002250031286774244, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0021958765192853637, 0.0012835592576171864, 0.00048152542855748855, 0.00027423419491804524, 0.0002145507775384476, 0.000136431746035496, 4.981242983661773e-05, 5.209962021900112e-05, 8.733972423518476e-05, 0.00019081080267606437, 0.0005185129464406754, 0.0011487157597858174, 0.0017513114594162626, 0.002149002406578356, 0.0022302646381919342, 0.0022578617718606478, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.002264868100464008, 0.0018877662878872517, 0.0011207148792924501, 0.0009062458863609847, 0.0010600816144021954, 0.001260831989027692, 0.0013444407164114804, 0.0010686180136602764, 0.0006063309483055392, 0.00038485271132143874, 0.00039282727913843367, 0.0007050179758789724, 0.00126740012988185, 0.0018183951608801811, 0.002094276923041832, 0.0021934019043175863, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.002103329723802333, 0.0018483268084508954, 0.0016948844902988963, 0.001746596708149173, 0.002010623613959107, 0.002174613565008838, 0.0021239874885492873, 0.002059558000272694, 0.0017682831626510353, 0.0012771022739579163, 0.0009620922113074756, 0.0010709700352637198, 0.0015126663545154825, 0.0019462854307141576, 0.002174613565008838, 0.0022474331808693838, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.002245704441065276, 0.0021078855367371156, 0.0020705127749715627, 0.002115522574493357, 0.002217559389596289, 0.0022526354066213913, 0.0022561169575854565, 0.002241394204601768, 0.0022234704433702164, 0.0021754237537596802, 0.0020508772912494057, 0.0017976812110477038, 0.0015763733432476706, 0.0016355411690438131, 0.001975923466232843, 0.0021967026322445384, 0.002260484059354882, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022294130939908502, 0.0022158762853535306, 0.0022396747391991503, 0.0022631124449967854, 0.0022683875960096394, 0.0022639899323660677, 0.0022561169575854565, 0.0022631124449967854, 0.0022543748379181446, 0.002244841068097983, 0.002177045943708313, 0.002164938161538321, 0.002102572335566923, 0.0021147563813558173, 0.002093526039260358, 0.0022091693402192733, 0.0022613595092805684, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.002264868100464008, 0.0022596092870003468, 0.002266626482016857, 0.0022710344042723954, 0.0022692691796621298, 0.002266626482016857, 0.002264868100464008, 0.002262235637564948, 0.002266626482016857, 0.0022657469500830473, 0.0022543748379181446, 0.0022158762853535306, 0.0022074989430052538, 0.0022192450526400736, 0.002235387595482995, 0.002258735191430638, 0.0022692691796621298, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022701514488164282, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022692691796621298, 0.0022692691796621298, 0.0022701514488164282, 0.0022710344042723954, 0.0022692691796621298, 0.0022683875960096394, 0.002267506697060341, 0.0022639899323660677, 0.002245704441065276, 0.0022474331808693838, 0.0022422549277471057, 0.0022657469500830473, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022692691796621298, 0.0022657469500830473, 0.0022692691796621298, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022701514488164282, 0.0022657469500830473, 0.0022613595092805684, 0.0022657469500830473, 0.002262235637564948, 0.0022422549277471057, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022701514488164282, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022692691796621298, 0.0022710344042723954, 0.0022701514488164282, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022701514488164282, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954]
RZHANG_LOSS_WEIGHT = 1000


def custom_loss_rebalancing_segments(y_true, y_pred):
    y_pred_clipped = K.clip(y_pred, K.epsilon(), None)
    print(y_pred.shape, y_true.shape)
    img_height = y_pred.shape[1]
    img_width = y_pred.shape[2]

    loss = -K.sum(y_true * K.log(y_pred_clipped), axis = 3)

    #bin_indices = K.argmax(y_true, axis = 3)
    #weights = K.gather(reference = AB_BIN_WEIGHTS, indices = bin_indices)

    #loss = RZHANG_LOSS_WEIGHT * K.sum(loss * weights)

    #loss += K.sum((y_pred[:, :-1, :, :] - y_pred[:, 1:, :, :]) ** 2)
    #loss += K.sum((y_pred[:, :, :-1, :] - y_pred[:, :, 1:, :]) ** 2)

    return loss


adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)
model_final.compile(optimizer=adam, loss=custom_loss_rebalancing_segments, metrics=[perceptual_distance])

print('Load validation set!!!')
(val_bw_images, val_color_images) = next(my_generator(145, val_dir))

print('Begin train!!')
model_final.fit_generator(my_generator(config.batch_size, train_dir),
                     steps_per_epoch=2,
                     epochs=config.num_epochs, callbacks=[WandbCallback(data_type='image', predictions=16)],
                     validation_data=(val_bw_images, val_color_images))


